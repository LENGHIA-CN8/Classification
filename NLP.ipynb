{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn hoá bảng mã tiếng việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicchar = loaddicchar()\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(data):\n",
    "    # chuẩn hoá unicode\n",
    "    data = convert_unicode(data)\n",
    "    # tách từ\n",
    "    data = word_tokenize(data, format=\"text\")\n",
    "    # xoá các kí tự \",\" \".\"  \n",
    "    data = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',data)\n",
    "    # xoá khoảng trắng\n",
    "    data = re.sub(r'\\s+', ' ', data).strip()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn bị Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Label in Trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('trainning.txt', 'r') \n",
    "\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "train_div = 0.8  # 8 phan cho train - 2 phan cho test\n",
    "Lines = file1.readlines()\n",
    "# count=0\n",
    "# Xu li phan label cho data\n",
    "for line in Lines:\n",
    "    words = text_preprocess(line).strip().split()\n",
    "    while words[0][-1] != \"_\":\n",
    "        words[0]=words[0][:-1:]\n",
    "    if words[0][-3] == \"_\":\n",
    "        words[0]=words[0][:-1:]\n",
    "    label.append(words[0])\n",
    "    text.append(' '.join(words[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__KH__      500\n",
       "__CTXH__    500\n",
       "__SK__      500\n",
       "__PL__      500\n",
       "__KD__      500\n",
       "__DS__      500\n",
       "__TT__      500\n",
       "__VT__      500\n",
       "__VH__      500\n",
       "__TG__      500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(text, label, test_size=train_div, random_state=21)\n",
    "df= pd.Series(label)\n",
    "df.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__CTXH__', '__DS__', '__KD__', '__KH__', '__PL__', '__SK__', '__TG__', '__TT__', '__VH__', '__VT__'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print(list(label_encoder.classes_), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 7, 1, 3, 0, 3, 7, 6, 9, 9, 6, 0, 6, 7, 8, 1, 1, 2, 8, 4, 4,\n",
       "       2, 1, 5, 0, 4, 9, 8, 6, 4, 0, 9, 1, 2, 2, 2, 1, 7, 9, 4, 4, 9, 9,\n",
       "       0, 3, 3, 1, 2, 1, 1, 2, 3, 1, 8, 9, 7, 4, 0, 0, 7, 6, 0, 9, 6, 7,\n",
       "       1, 4, 8, 1, 1, 5, 6, 3, 6, 1, 3, 8, 9, 2, 2, 2, 5, 5, 8, 5, 9, 5,\n",
       "       8, 4, 0, 8, 4, 9, 4, 0, 3, 8, 5, 3, 0, 1, 8, 6, 0, 3, 1, 5, 9, 4,\n",
       "       1, 8, 0, 2, 3, 3, 7, 2, 7, 1, 3, 4, 5, 7, 1, 9, 6, 7, 8, 6, 5, 7,\n",
       "       0, 1, 5, 0, 8, 7, 3, 8, 6, 4, 9, 5, 0, 0, 7, 1, 9, 7, 8, 7, 8, 2,\n",
       "       0, 8, 5, 7, 9, 2, 9, 4, 6, 3, 8, 1, 7, 3, 3, 7, 5, 1, 5, 6, 9, 8,\n",
       "       9, 7, 5, 1, 7, 6, 5, 4, 6, 7, 5, 9, 4, 8, 5, 2, 4, 0, 5, 4, 9, 7,\n",
       "       8, 5, 7, 1, 3, 6, 5, 1, 6, 9, 9, 8, 3, 4, 0, 8, 3, 6, 7, 7, 0, 1,\n",
       "       0, 7, 4, 8, 3, 7, 4, 0, 2, 8, 3, 0, 2, 4, 9, 5, 4, 6, 4, 8, 5, 6,\n",
       "       6, 3, 2, 9, 2, 1, 7, 2, 5, 8, 2, 3, 3, 0, 8, 9, 3, 8, 1, 3, 7, 2,\n",
       "       8, 3, 8, 7, 1, 9, 5, 4, 3, 0, 6, 5, 8, 5, 1, 4, 0, 2, 7, 3, 5, 7,\n",
       "       5, 0, 2, 3, 8, 1, 8, 5, 2, 7, 4, 2, 9, 4, 5, 9, 1, 1, 5, 0, 6, 2,\n",
       "       7, 5, 2, 6, 1, 4, 1, 5, 4, 0, 8, 8, 9, 2, 1, 0, 8, 2, 5, 0, 6, 0,\n",
       "       3, 7, 5, 3, 5, 7, 7, 2, 7, 7, 0, 9, 2, 9, 2, 2, 5, 0, 4, 8, 6, 2,\n",
       "       1, 8, 5, 6, 8, 7, 8, 1, 7, 2, 3, 3, 7, 2, 0, 8, 1, 8, 1, 5, 6, 5,\n",
       "       5, 2, 9, 7, 9, 2, 6, 0, 4, 5, 2, 4, 3, 0, 2, 9, 7, 8, 3, 2, 6, 1,\n",
       "       8, 7, 1, 0, 6, 9, 3, 4, 2, 9, 4, 1, 8, 1, 2, 8, 2, 4, 8, 5, 4, 5,\n",
       "       6, 3, 0, 7, 5, 0, 8, 4, 0, 3, 6, 4, 1, 5, 5, 2, 0, 5, 3, 4, 4, 4,\n",
       "       1, 1, 6, 6, 1, 7, 1, 3, 6, 7, 5, 5, 7, 6, 7, 8, 4, 4, 2, 3, 0, 1,\n",
       "       8, 2, 2, 8, 3, 7, 4, 0, 8, 0, 5, 3, 4, 1, 2, 1, 7, 7, 6, 7, 0, 6,\n",
       "       0, 4, 5, 7, 9, 6, 0, 7, 1, 3, 5, 0, 2, 7, 7, 7, 1, 4, 1, 7, 0, 4,\n",
       "       4, 9, 2, 1, 3, 2, 7, 8, 7, 1, 9, 4, 0, 2, 2, 8, 4, 0, 2, 4, 0, 7,\n",
       "       1, 4, 8, 9, 1, 4, 6, 1, 5, 6, 8, 5, 5, 1, 6, 5, 3, 2, 0, 1, 0, 3,\n",
       "       4, 8, 2, 0, 6, 5, 9, 7, 5, 8, 9, 8, 3, 0, 4, 3, 3, 8, 1, 5, 9, 6,\n",
       "       5, 7, 3, 1, 8, 5, 4, 7, 7, 4, 6, 5, 2, 5, 4, 4, 9, 9, 8, 7, 3, 6,\n",
       "       6, 5, 1, 5, 6, 2, 2, 6, 9, 6, 9, 9, 6, 9, 9, 3, 4, 5, 8, 5, 0, 6,\n",
       "       8, 0, 7, 8, 5, 9, 9, 2, 0, 8, 2, 4, 6, 1, 4, 9, 7, 3, 2, 3, 9, 9,\n",
       "       7, 8, 8, 3, 9, 7, 4, 5, 3, 3, 0, 2, 0, 5, 2, 6, 3, 1, 4, 6, 3, 0,\n",
       "       5, 4, 0, 8, 0, 3, 6, 1, 6, 4, 8, 8, 7, 7, 2, 2, 9, 8, 4, 1, 8, 7,\n",
       "       9, 5, 2, 3, 0, 3, 9, 8, 0, 9, 1, 1, 0, 5, 8, 1, 4, 9, 0, 4, 5, 2,\n",
       "       3, 3, 9, 7, 1, 6, 4, 4, 2, 8, 6, 8, 6, 6, 5, 0, 5, 6, 6, 9, 6, 4,\n",
       "       9, 2, 4, 0, 8, 6, 2, 9, 5, 3, 8, 2, 6, 9, 5, 5, 8, 5, 9, 7, 7, 1,\n",
       "       3, 3, 9, 4, 4, 0, 8, 0, 5, 9, 0, 0, 7, 7, 6, 2, 0, 8, 3, 1, 2, 2,\n",
       "       9, 5, 2, 0, 7, 8, 4, 6, 3, 8, 9, 7, 0, 7, 2, 3, 0, 7, 3, 1, 8, 9,\n",
       "       2, 0, 2, 5, 5, 6, 0, 3, 0, 6, 0, 2, 1, 1, 6, 0, 3, 5, 4, 2, 1, 8,\n",
       "       4, 6, 3, 2, 2, 7, 3, 9, 6, 1, 8, 1, 4, 2, 6, 2, 6, 2, 0, 7, 0, 5,\n",
       "       9, 1, 1, 3, 3, 0, 7, 2, 1, 4, 1, 7, 8, 5, 1, 8, 0, 8, 7, 2, 7, 0,\n",
       "       6, 0, 0, 7, 1, 7, 1, 3, 4, 7, 7, 8, 3, 5, 6, 5, 7, 4, 8, 1, 6, 2,\n",
       "       8, 3, 1, 7, 1, 1, 3, 9, 5, 6, 8, 6, 4, 6, 4, 0, 8, 6, 1, 4, 3, 0,\n",
       "       6, 1, 1, 3, 3, 0, 6, 1, 4, 7, 0, 5, 8, 2, 6, 8, 0, 2, 5, 3, 0, 3,\n",
       "       4, 7, 8, 5, 7, 7, 1, 6, 1, 8, 4, 5, 9, 2, 3, 4, 4, 5, 3, 6, 5, 3,\n",
       "       9, 0, 7, 0, 4, 5, 2, 4, 8, 6, 1, 3, 6, 3, 1, 9, 2, 4, 2, 6, 0, 4,\n",
       "       8, 9, 7, 5, 5, 7, 7, 1, 3, 1, 5, 2, 1, 0, 5, 6, 3, 3, 3, 4, 7, 1,\n",
       "       0, 5, 0, 1, 5, 2, 8, 6, 0, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()), # vector hoá từ \n",
    "                     ('tfidf', TfidfTransformer()), #Tf-idf từ\n",
    "                     ('clf-svm', SVC()) # SVC\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 2, ..., 9, 8, 7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8195"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pre,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open('testing.txt','r')\n",
    "\n",
    "\n",
    "text2 = []\n",
    "Lines = file1.readlines()\n",
    "Lines2 = file2.readlines()\n",
    "# count=0\n",
    "# Xu li phan label cho data\n",
    "for line in Lines2:\n",
    "    words = text_preprocess(line).strip().split()\n",
    "    text2.append(' '.join(words[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 4, ..., 5, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_pre = label_encoder.inverse_transform(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['__KD__', '__DS__', '__PL__', ..., '__SK__', '__CTXH__',\n",
       "       '__CTXH__'], dtype='<U8')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = open('predicts.txt', 'w') \n",
    "for label in y_label_pre :\n",
    "    file3.write(label)\n",
    "    file3.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[329,  13,  18,   5,   9,   9,   1,   0,   9,   1],\n",
       "       [ 31, 314,   4,  13,   0,  13,   0,   3,  16,   0],\n",
       "       [ 54,   2, 324,   5,   1,   0,   3,   0,   0,  12],\n",
       "       [ 35,  27,   0, 316,   0,  16,   2,   0,   1,   4],\n",
       "       [ 69,   9,   4,   0, 312,   2,   2,   1,   2,   1],\n",
       "       [ 15,   6,   5,  10,   0, 356,   0,   0,   0,   1],\n",
       "       [ 34,   5,  10,  12,   2,  10, 331,   0,   1,   1],\n",
       "       [ 27,   2,   2,   0,   2,   2,   2, 353,   3,   1],\n",
       "       [ 32,  13,   1,   4,   1,   0,   3,   0, 342,   0],\n",
       "       [ 59,   8,  18,  23,   1,   1,   3,   0,   5, 301]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
